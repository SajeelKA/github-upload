{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from random import randrange\n",
    "from IPython.display import YouTubeVideo\n",
    "import keras_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, change the newPath variable below to point to the location of the tfrecords file, then run all of the cells in order to get the results. You can also comment and uncomment some parts to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPath = \"./audioset_v1_embeddings/myFiltered/\"\n",
    "files = os.listdir( newPath )\n",
    "\n",
    "df = pd.DataFrame(columns=[\"ytid\", \"Speech\", \"Animal\", \"Music\", \"Vehicle\"])\n",
    "\n",
    "oneFile=[]\n",
    "allFiles=[]\n",
    "myLabels=[]\n",
    "myLabelsAll=[]\n",
    "multiLabels=[]\n",
    "ytid=[]\n",
    "ytidMulti=[]\n",
    "count=0;\n",
    "ind=0;\n",
    "\n",
    "for file in files:\n",
    "    secCount=0\n",
    "    \n",
    "    tfrecords_filename = newPath+file\n",
    "\n",
    "    record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "\n",
    "    string_record = next(record_iterator)\n",
    "\n",
    "    example = tf.train.SequenceExample()\n",
    "\n",
    "    example.ParseFromString(string_record)    \n",
    "    \n",
    "    #to store the id of the video for later use\n",
    "    vidId=example.context.feature['video_id'].bytes_list.value[0].decode()\n",
    "    \n",
    "    label=example.context.feature['labels'].int64_list.value[:]       \n",
    "    \n",
    "    for a in example.feature_lists.feature_list['audio_embedding'].feature:\n",
    "        secCount+=1        \n",
    "    #print(secCount)\n",
    "    \n",
    "    if secCount == 10: # make sure all samples are same length\n",
    "        if 0 in label or 72 in label or 137 in label or 300 in label:\n",
    "            df.loc[ind, ['ytid']]=vidId\n",
    "            #df=df.append({\"ytid\":vidId, \"Speech\":0, \"Animal\":0, \"Music\":0, \"Vehicle\":0}, ignore_index=True)\n",
    "\n",
    "            if 0 in label:\n",
    "                df.loc[ind, ['Speech']]=1\n",
    "            #    df.iloc[ind,1]=1\n",
    "            else:\n",
    "                df.loc[ind, ['Speech']]=0\n",
    "                #df.iloc[ind,1]=0\n",
    "            if 72 in label:\n",
    "                df.loc[ind, ['Animal']]=1\n",
    "                #df.iloc[ind,2]=1\n",
    "            else:\n",
    "                df.loc[ind, ['Animal']]=0\n",
    "               #df.iloc[ind,2]=0\n",
    "            if 137 in label:\n",
    "                df.loc[ind, ['Music']]=1\n",
    "                #df.iloc[ind,3]=1\n",
    "            else:\n",
    "                df.loc[ind, ['Music']]=0            \n",
    "                #df.iloc[ind,3]=0\n",
    "            if 300 in label:\n",
    "                df.loc[ind, ['Vehicle']]=1\n",
    "                #df.iloc[ind,4]=1\n",
    "            else:\n",
    "                df.loc[ind, ['Vehicle']]=0\n",
    "                #df.iloc[ind,4]=0\n",
    "\n",
    "            ind+=1\n",
    "\n",
    "            for a in example.feature_lists.feature_list['audio_embedding'].feature:\n",
    "                    # 960ms of data\n",
    "                hexembed = a.bytes_list.value[0].hex()\n",
    "\n",
    "                arrayembed = [int(hexembed[i:i+2],16) for i in range(0,len(hexembed),2)]         \n",
    "\n",
    "                allFiles.append(arrayembed)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forSpeech(data):    \n",
    "    \n",
    "    model=Conv2D(32, (3, 3), padding=\"same\")(data)\n",
    "    #model=Conv2D(16, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(data)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    \n",
    "    model=Flatten()(model)\n",
    "    model=Dense(1000, activation='relu')(model)\n",
    "    #model=Dense(1, activation='softmax', name=\"speech_output\")(model)\n",
    "    model=Dense(2, activation='softmax', name=\"speech_output\")(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def forAnimal(data):\n",
    "    \n",
    "    model=Conv2D(16, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(data)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    \n",
    "    model=Flatten()(model)\n",
    "    model=Dense(1000, activation='relu')(model)\n",
    "    #model=Dense(1, activation='softmax', name=\"animal_output\")(model)\n",
    "    model=Dense(2, activation='softmax', name=\"animal_output\")(model)\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "def forMusic(data):\n",
    "    \n",
    "    model=Conv2D(16, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(data)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    \n",
    "    model=Flatten()(model)\n",
    "    model=Dense(1000, activation='relu')(model)\n",
    "    #model=Dense(1, activation='softmax', name=\"music_output\")(model)   \n",
    "    model=Dense(2, activation='softmax', name=\"music_output\")(model)   \n",
    "    \n",
    "    return model\n",
    "\n",
    "def forVehicle(data):\n",
    "    \n",
    "    model=Conv2D(16, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(data)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    model=MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(model)\n",
    "    \n",
    "    model=Conv2D(32, kernel_size=(1, 6), strides=(2, 2), activation='relu',input_shape=(10,128,1))(model)\n",
    "    \n",
    "    model=Flatten()(model)\n",
    "    model=Dense(1000, activation='relu')(model)\n",
    "    #model=Dense(1, activation='softmax', name=\"vehicle_output\")(model)    \n",
    "    model=Dense(2, activation='softmax', name=\"vehicle_output\")(model)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forSpeechONN(data):        \n",
    "    \n",
    "    model2 = Dense(10, activation='relu',input_shape=(1280,))(data)\n",
    "    model2 = Dense(100, activation='relu')(model2)\n",
    "    model2 = Dense(1000, activation='relu')(model2)\n",
    "    #model2 = Dense(1, activation='softmax', name=\"speech_output2\")(model2)\n",
    "    model2 = Dense(2, activation='softmax', name=\"speech_output2\")(model2)\n",
    "\n",
    "    return model2\n",
    "\n",
    "\n",
    "def forAnimalONN(data):    \n",
    "    \n",
    "    model2 = Dense(10, activation='relu',input_shape=(1280,))(data)\n",
    "    model2 = Dense(100, activation='relu')(model2)\n",
    "    model2 = Dense(1000, activation='relu')(model2)\n",
    "    #model2 = Dense(1, activation='softmax', name=\"animal_output2\")(model2)\n",
    "    model2 = Dense(2, activation='softmax', name=\"animal_output2\")(model2)\n",
    "\n",
    "    return model2\n",
    "\n",
    "def forMusicONN(data):    \n",
    "    \n",
    "    model2 = Dense(10, activation='relu',input_shape=(1280,))(data)\n",
    "    model2 = Dense(100, activation='relu')(model2)\n",
    "    model2 = Dense(1000, activation='relu')(model2)\n",
    "    #model2 = Dense(1, activation='softmax', name=\"music_output2\")(model2)\n",
    "    model2 = Dense(2, activation='softmax', name=\"music_output2\")(model2)\n",
    "\n",
    "    return model2\n",
    "\n",
    "def forVehicleONN(data):    \n",
    "    \n",
    "    model2 = Dense(10, activation='relu',input_shape=(1280,))(data)\n",
    "    model2 = Dense(100, activation='relu')(model2)\n",
    "    model2 = Dense(1000, activation='relu')(model2)\n",
    "    #model2 = Dense(1, activation='softmax', name=\"vehicle_output2\")(model2)\n",
    "    model2 = Dense(2, activation='softmax', name=\"vehicle_output2\")(model2)\n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffling(allSamples,labels,sampleToShuffle, vidId, option):\n",
    "    X=allSamples    \n",
    "    Y=labels\n",
    "    x1=sampleToShuffle\n",
    "    #y1=sampleLabel\n",
    "    \n",
    "    r=[randrange(10) for _ in range(0,10)]\n",
    "    for num in range(10):\n",
    "        if(num < int(len(r)/2)):\n",
    "            #print('1st is {0}, 2nd is {1}'.format(x[(len(x)-1)-num],num))\n",
    "            temp=x1[num].copy()\n",
    "            x1[num]=x1[r[num]].copy()#r[num] is the index num in the list r of random numbers between 1 and 10\n",
    "            x1[r[num]]=temp.copy()    \n",
    "\n",
    "\n",
    "    X=np.append(X, x1.reshape(1,10,128,1), axis=0) \n",
    "    \n",
    "    if option==1:#value to add to is Animal\n",
    "        Y=Y.append({\"ytid\":vidId, \"Speech\":0, \"Animal\":1, \"Music\":0, \"Vehicle\":0}, ignore_index=True)\n",
    "    if option==2:#value to add to is Vehicle\n",
    "        Y=Y.append({\"ytid\":vidId, \"Speech\":0, \"Animal\":0, \"Music\":0, \"Vehicle\":1}, ignore_index=True)\n",
    "\n",
    "    #print(len(X))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(allFiles)\n",
    "X=X.reshape(int(len(X)/10),10,128,1) # no of 10 s clips, first dimension, 2nd dimension, 1 channel\n",
    "\n",
    "count72=0\n",
    "count300=0\n",
    "\n",
    "while count72<=300: # count72 is for Animal label\n",
    "    count72=0\n",
    "    count300=0\n",
    "    for i, x in enumerate(X):\n",
    "        if df.iloc[i, 2] == 1 and count72<=300: #df.iloc[i,2] is animal\n",
    "            count72+=1\n",
    "            X, df =shuffling(X, df, X[i],df.iloc[i, 0], 1)\n",
    "            #ytidNew.append(ytidNew[i])\n",
    "        if df.iloc[i, 4] == 1 and count300<=300:#df.iloc[i,2] is vehicle\n",
    "            count300+=1\n",
    "            X, df =shuffling(X, df, X[i], df.iloc[i, 0], 2)\n",
    "            #ytidNew.append(ytidNew[i])\n",
    "    if count72>300:\n",
    "        break\n",
    "\n",
    "X=X/255 # for kind of normalization\n",
    "\n",
    "np.save('XFullForMulti',X)\n",
    "df.to_csv('theLabels.csv')\n",
    "\n",
    "#Xt = tf.convert_to_tensor(X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "myInput=Input(shape=(10,128,1))\n",
    "\n",
    "#Xt=Input(shape=Xt.shape[1:4])\n",
    "\n",
    "sp = forSpeech(myInput)\n",
    "an = forAnimal(myInput)\n",
    "mu = forMusic(myInput)\n",
    "ve = forVehicle(myInput)\n",
    "\n",
    "theClasses = [sp,an,mu,ve]\n",
    "\n",
    "multiModel = Model(inputs=myInput, outputs=theClasses)\n",
    "\n",
    "# losses = {\n",
    "# \"speech_output\": \"categorical_crossentropy\",\n",
    "# \"animal_output\": \"categorical_crossentropy\",\n",
    "# \"music_output\": \"categorical_crossentropy\",\n",
    "# \"vehicle_output\": \"categorical_crossentropy\"\n",
    "# }\n",
    "\n",
    "# losses = {\n",
    "# \"speech_output\": \"binary_crossentropy\",\n",
    "# \"animal_output\": \"binary_crossentropy\",\n",
    "# \"music_output\": \"binary_crossentropy\",\n",
    "# \"vehicle_output\": \"binary_crossentropy\"\n",
    "# }\n",
    "\n",
    "losses = {\n",
    "\"speech_output\": \"sparse_categorical_crossentropy\",\n",
    "\"animal_output\": \"sparse_categorical_crossentropy\",\n",
    "\"music_output\": \"sparse_categorical_crossentropy\",\n",
    "\"vehicle_output\": \"sparse_categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "\n",
    "# initialize the optimizer and compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "multiModel.compile(optimizer='adam', loss=losses, metrics=[keras_metrics.precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model for ONN...\n"
     ]
    }
   ],
   "source": [
    "X2=X.reshape(len(X), 10*128)\n",
    "\n",
    "\n",
    "myInputONN=Input(shape=(10*128,))\n",
    "\n",
    "spONN = forSpeechONN(myInputONN)\n",
    "anONN = forAnimalONN(myInputONN)\n",
    "muONN = forMusicONN(myInputONN)\n",
    "veONN = forVehicleONN(myInputONN)\n",
    "\n",
    "# losses2 = {\n",
    "# \"speech_output2\": \"binary_crossentropy\",\n",
    "# \"animal_output2\": \"binary_crossentropy\",\n",
    "# \"music_output2\": \"binary_crossentropy\",\n",
    "# \"vehicle_output2\": \"binary_crossentropy\"\n",
    "# }\n",
    "\n",
    "losses2 = {\n",
    "\"speech_output2\": \"sparse_categorical_crossentropy\",\n",
    "\"animal_output2\": \"sparse_categorical_crossentropy\",\n",
    "\"music_output2\": \"sparse_categorical_crossentropy\",\n",
    "\"vehicle_output2\": \"sparse_categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "theClassesONN = [spONN,anONN,muONN,veONN]\n",
    "\n",
    "multiModelONN = Model(inputs=myInputONN, outputs=theClassesONN)\n",
    "print(\"[INFO] compiling model for ONN...\")\n",
    "multiModelONN.compile(optimizer='adam', loss=losses2, metrics=[keras_metrics.precision()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4059/4059 [==============================] - 9s 2ms/step - loss: 1.7496 - speech_output_loss: 0.4077 - animal_output_loss: 0.3152 - music_output_loss: 0.4930 - vehicle_output_loss: 0.5337 - speech_output_precision: 0.1899 - animal_output_precision: 0.1898 - music_output_precision: 0.1899 - vehicle_output_precision: 0.1902\n",
      "Epoch 2/5\n",
      "4059/4059 [==============================] - 10s 2ms/step - loss: 0.7085 - speech_output_loss: 0.1913 - animal_output_loss: 0.0853 - music_output_loss: 0.2151 - vehicle_output_loss: 0.2167 - speech_output_precision: 0.0530 - animal_output_precision: 0.0530 - music_output_precision: 0.0528 - vehicle_output_precision: 0.0529\n",
      "Epoch 3/5\n",
      "4059/4059 [==============================] - 9s 2ms/step - loss: 0.4043 - speech_output_loss: 0.1106 - animal_output_loss: 0.0491 - music_output_loss: 0.1293 - vehicle_output_loss: 0.1154 - speech_output_precision: 0.0265 - animal_output_precision: 0.0265 - music_output_precision: 0.0265 - vehicle_output_precision: 0.0265\n",
      "Epoch 4/5\n",
      "4059/4059 [==============================] - 12s 3ms/step - loss: 0.2721 - speech_output_loss: 0.0741 - animal_output_loss: 0.0280 - music_output_loss: 0.0913 - vehicle_output_loss: 0.0787 - speech_output_precision: 0.0167 - animal_output_precision: 0.0167 - music_output_precision: 0.0168 - vehicle_output_precision: 0.0168\n",
      "Epoch 5/5\n",
      "4059/4059 [==============================] - 9s 2ms/step - loss: 0.2133 - speech_output_loss: 0.0647 - animal_output_loss: 0.0180 - music_output_loss: 0.0696 - vehicle_output_loss: 0.0611 - speech_output_precision: 0.0124 - animal_output_precision: 0.0124 - music_output_precision: 0.0124 - vehicle_output_precision: 0.0127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97ef071ef0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "multiModel.fit(X,{\"speech_output\": df['Speech'], \"animal_output\": df['Animal'], \"music_output\": df['Music'], \"vehicle_output\": df['Vehicle']}, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4059/4059 [==============================] - 1s 321us/step - loss: 1.3638 - speech_output2_loss: 0.4179 - animal_output2_loss: 0.2303 - music_output2_loss: 0.4526 - vehicle_output2_loss: 0.2630 - speech_output2_precision: 0.1492 - animal_output2_precision: 0.1493 - music_output2_precision: 0.1492 - vehicle_output2_precision: 0.1491\n",
      "Epoch 2/5\n",
      "4059/4059 [==============================] - 1s 235us/step - loss: 0.5894 - speech_output2_loss: 0.2108 - animal_output2_loss: 0.0992 - music_output2_loss: 0.1699 - vehicle_output2_loss: 0.1096 - speech_output2_precision: 0.0394 - animal_output2_precision: 0.0393 - music_output2_precision: 0.0393 - vehicle_output2_precision: 0.0394\n",
      "Epoch 3/5\n",
      "4059/4059 [==============================] - 1s 235us/step - loss: 0.4223 - speech_output2_loss: 0.1760 - animal_output2_loss: 0.0643 - music_output2_loss: 0.0985 - vehicle_output2_loss: 0.0834 - speech_output2_precision: 0.0266 - animal_output2_precision: 0.0266 - music_output2_precision: 0.0266 - vehicle_output2_precision: 0.0267\n",
      "Epoch 4/5\n",
      "4059/4059 [==============================] - 1s 242us/step - loss: 0.2944 - speech_output2_loss: 0.1348 - animal_output2_loss: 0.0390 - music_output2_loss: 0.0659 - vehicle_output2_loss: 0.0547 - speech_output2_precision: 0.0162 - animal_output2_precision: 0.0161 - music_output2_precision: 0.0161 - vehicle_output2_precision: 0.0161\n",
      "Epoch 5/5\n",
      "4059/4059 [==============================] - 1s 241us/step - loss: 0.3022 - speech_output2_loss: 0.1189 - animal_output2_loss: 0.0510 - music_output2_loss: 0.0625 - vehicle_output2_loss: 0.0698 - speech_output2_precision: 0.0183 - animal_output2_precision: 0.0183 - music_output2_precision: 0.0183 - vehicle_output2_precision: 0.0183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9765bd6d30>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiModelONN.fit(X2,{\"speech_output2\": df['Speech'], \"animal_output2\": df['Animal'], \"music_output2\": df['Music'], \"vehicle_output2\": df['Vehicle']}, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.92138237, 0.07861765],\n",
      "       [0.9422893 , 0.05771071],\n",
      "       [0.8581353 , 0.1418647 ],\n",
      "       [0.98682433, 0.01317559],\n",
      "       [0.75007665, 0.2499234 ],\n",
      "       [0.18616736, 0.8138326 ],\n",
      "       [0.94923705, 0.05076298],\n",
      "       [0.94348973, 0.0565103 ],\n",
      "       [0.02335717, 0.97664285],\n",
      "       [0.52039164, 0.47960836]], dtype=float32), array([[1.0000000e+00, 4.9295884e-10],\n",
      "       [1.0000000e+00, 4.8987072e-09],\n",
      "       [1.0000000e+00, 2.3548710e-10],\n",
      "       [1.0000000e+00, 1.5121419e-10],\n",
      "       [1.0000000e+00, 1.0007816e-09],\n",
      "       [8.6159927e-01, 1.3840076e-01],\n",
      "       [9.9999988e-01, 6.1597177e-08],\n",
      "       [9.9998045e-01, 1.9555628e-05],\n",
      "       [9.9993777e-01, 6.2213971e-05],\n",
      "       [9.9999976e-01, 1.9311015e-07]], dtype=float32), array([[3.4462530e-03, 9.9655378e-01],\n",
      "       [4.6916339e-03, 9.9530834e-01],\n",
      "       [1.3920541e-01, 8.6079454e-01],\n",
      "       [2.7713964e-03, 9.9722856e-01],\n",
      "       [9.9993789e-01, 6.2078485e-05],\n",
      "       [9.9979395e-01, 2.0605672e-04],\n",
      "       [9.9998319e-01, 1.6777200e-05],\n",
      "       [4.9662998e-01, 5.0337005e-01],\n",
      "       [9.9976319e-01, 2.3680656e-04],\n",
      "       [2.7713964e-03, 9.9722856e-01]], dtype=float32), array([[9.99998689e-01, 1.29417936e-06],\n",
      "       [1.00000000e+00, 5.51806139e-08],\n",
      "       [9.99999404e-01, 6.29262274e-07],\n",
      "       [9.99642134e-01, 3.57912155e-04],\n",
      "       [1.35240415e-02, 9.86476004e-01],\n",
      "       [9.99983191e-01, 1.67882445e-05],\n",
      "       [1.87519249e-02, 9.81248021e-01],\n",
      "       [9.99907851e-01, 9.20809471e-05],\n",
      "       [1.00000000e+00, 3.59954413e-08],\n",
      "       [9.99998569e-01, 1.47246146e-06]], dtype=float32)]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "Name: Speech, dtype: object\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "Name: Animal, dtype: object\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "Name: Music, dtype: object\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "Name: Vehicle, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pred = multiModelONN.predict(X2[:10])\n",
    "print(pred)\n",
    "print(df['Speech'][0:3])\n",
    "print(df['Animal'][0:3])\n",
    "print(df['Music'][0:3])\n",
    "print(df['Vehicle'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
